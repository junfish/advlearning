{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_code.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPad1Dwiochk/Tor8SLhnI+"},"kernelspec":{"display_name":"Python 3","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qvaQMRercMys","executionInfo":{"status":"ok","timestamp":1616065416814,"user_tz":-480,"elapsed":93977,"user":{"displayName":"Jun Yu","photoUrl":"","userId":"03149508337013056244"}},"outputId":"7e2a642e-0a58-4f0d-f3e4-6a1b15db378a"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/', force_remount=True)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BgE7XNPcw3m","executionInfo":{"status":"ok","timestamp":1616084888206,"user_tz":-480,"elapsed":1378,"user":{"displayName":"Jun Yu","photoUrl":"","userId":"03149508337013056244"}},"outputId":"8dd8728d-318b-43f4-e7c6-ef28f4e84490"},"source":["%cd /content/gdrive/MyDrive/CSE-498/Adversarial_Learning\n","!ls\n","!nvidia-smi"],"execution_count":59,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/CSE-498/Adversarial_Learning\n","attacking_main.py  datasets\t\t   Middle_Results  README.md\n","base_model.py\t   experiment_operator.py  model_weights   run_code.ipynb\n","data_process.py    learning_main.py\t   __pycache__\n","Thu Mar 18 16:28:07 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.56       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla V100-SXM2...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   35C    P0    36W / 300W |   1897MiB / 16160MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":725},"id":"8w4dsHzqiap7","executionInfo":{"status":"ok","timestamp":1616085238513,"user_tz":-480,"elapsed":1893,"user":{"displayName":"Jun Yu","photoUrl":"","userId":"03149508337013056244"}},"outputId":"778bfbf2-5466-42db-e43f-d95dbfe05cd6"},"source":["# 26, 226 426-7 912\n","# model dataset --lr --norm --batch_size --sample --target\n","%run attacking_main.py 0 0 -l 1e-1 -n 1 -b 100 -s 1000"],"execution_count":69,"outputs":[{"output_type":"stream","text":["Loading mnist to the memory...\n","Initialize 2 Layer CNN...\n","The chosen picture's idx and class are as below:\n","tensor(9)\n","9 - nine\n","Visualize this picture:\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/content/gdrive/My Drive/CSE-498/Adversarial_Learning/attacking_main.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m                                         \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                                         \u001b[0mis_BN\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                                         is_gpu=True)\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading learned parameters.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# \"model_weights/cifar10-imagenet-resnet18.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/My Drive/CSE-498/Adversarial_Learning/experiment_operator.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, datasets_loaders, norm, model, batch_size, milestones, target_parameters, lr, scale, is_BN, is_gpu)\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_VISIBLE_DEVICES\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mxpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tRRT0_7thzfR","executionInfo":{"status":"ok","timestamp":1616062338380,"user_tz":-480,"elapsed":3035,"user":{"displayName":"Jun Yu","photoUrl":"","userId":"03149508337013056244"}},"outputId":"b2873269-4934-439e-f086-cd53ddcaf6fd"},"source":["!python learning_main.py --help"],"execution_count":4,"outputs":[{"output_type":"stream","text":["usage: learning_main.py [-h] [-l LR] [-n NORM] [-b BATCH_SIZE] [-i ITERATIONS]\n","                        [-s SCHEDULE_INTERVALS [SCHEDULE_INTERVALS ...]]\n","                        {0,1} {0,1}\n","\n","This is the main training program.\n","Firstly, You can specify the dataset and network for the learning process.\n","Secondly, You can also set hyper-parameters for your model training.\n","More details can be seen from the introduction of arguments below .\n","\n","positional arguments:\n","  {0,1}                 specify a deep learning model (1 as default)\n","                        0: LeNet (a CNN with two conv layers)\n","                        1: ResNet18 (change the first conv layer a little bit)\n","  {0,1}                 specify a dataset to learn (1 as default)\n","                        0: mnist\n","                        1: cifar10\n","\n","optional arguments:\n","  -h, --help            show this help message and exit\n","  -l LR, --lr LR        give an initial learning rate to our model\n","  -n NORM, --norm NORM  choose the norm for normalization\n","                        0: imagenet mean and std\n","                        1: compute own mean and std\n","  -b BATCH_SIZE, --batch_size BATCH_SIZE\n","                        set a batch size for the training and testing process\n","  -i ITERATIONS, --iterations ITERATIONS\n","                        the total number of epoch for the training\n","  -s SCHEDULE_INTERVALS [SCHEDULE_INTERVALS ...], --schedule_intervals SCHEDULE_INTERVALS [SCHEDULE_INTERVALS ...]\n","                        intervals to degrade learning rate\n","                        e.x., recommendation: start at lr = 0.1, intervals = [135, 230, 300], divided by 10, for cifar10 & resnet18\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cqLvIaTGc71t","executionInfo":{"status":"ok","timestamp":1616072917086,"user_tz":-480,"elapsed":980374,"user":{"displayName":"Jun Yu","photoUrl":"","userId":"03149508337013056244"}},"outputId":"441604e1-6ae2-4c4b-c298-f9af33c36b16"},"source":["# model dataset --lr --norm --batch_size --iterations --schedule_intervals\n","!python learning_main.py 1 1 -l 0.1 -n 1 -b 100 -i 400 -s 135 230 300"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Namespace(batch_size=100, dataset=1, iterations=200, lr=0.1, model=0, norm=1, schedule_intervals=[80, 130, 170])\n","Loading cifar10 to the memory.\n","Initializing 2 Layer CNN...\n","tcmalloc: large alloc 1228800000 bytes == 0x55e43c3d0000 @  0x7f1a11d9d1e7 0x7f19bdbf346e 0x7f19bdc43c7b 0x7f19bdc43ebe 0x7f19bdcdc887 0x55e424e3e050 0x55e424f2f99d 0x55e424eb1fe9 0x55e424e3f69a 0x55e424eadc9e 0x55e424eacb0e 0x55e424eac813 0x55e424f76592 0x55e424f7690d 0x55e424f767b6 0x55e424f4e103 0x55e424f4ddac 0x7f1a10b87bf7 0x55e424f4dc8a\n","tcmalloc: large alloc 1228800000 bytes == 0x55e43c3d0000 @  0x7f1a11d9d1e7 0x7f19bdbf346e 0x7f19bdc43c7b 0x7f19bdc43ebe 0x7f19bdcdc887 0x55e424e3e050 0x55e424f2f99d 0x55e424eb1fe9 0x55e424e3f69a 0x55e424eadc9e 0x55e424eacb0e 0x55e424eac813 0x55e424f76592 0x55e424f7690d 0x55e424f767b6 0x55e424f4e103 0x55e424f4ddac 0x7f1a10b87bf7 0x55e424f4dc8a\n","Start training...\n","Learning Rate = 0.10000000\n","Epoch 001: training_accuracy = 0.2995, testing_accuracy = 0.3147\n","           training_loss = 0.018374, testing_loss = 0.017953\n","Learning Rate = 0.10000000\n","Epoch 002: training_accuracy = 0.3493, testing_accuracy = 0.3749\n","           training_loss = 0.017652, testing_loss = 0.016948\n","Learning Rate = 0.10000000\n","Epoch 003: training_accuracy = 0.3334, testing_accuracy = 0.3595\n","           training_loss = 0.017689, testing_loss = 0.017139\n","Learning Rate = 0.10000000\n","Epoch 004: training_accuracy = 0.3376, testing_accuracy = 0.3556\n","           training_loss = 0.017806, testing_loss = 0.017535\n","Learning Rate = 0.10000000\n","Epoch 005: training_accuracy = 0.3460, testing_accuracy = 0.3604\n","           training_loss = 0.017731, testing_loss = 0.017278\n","Learning Rate = 0.10000000\n","Epoch 006: training_accuracy = 0.3554, testing_accuracy = 0.3852\n","           training_loss = 0.017862, testing_loss = 0.017322\n","Learning Rate = 0.10000000\n","Epoch 007: training_accuracy = 0.3737, testing_accuracy = 0.3898\n","           training_loss = 0.016900, testing_loss = 0.016530\n","Learning Rate = 0.10000000\n","Epoch 008: training_accuracy = 0.3726, testing_accuracy = 0.3816\n","           training_loss = 0.017264, testing_loss = 0.017100\n","Learning Rate = 0.10000000\n","Epoch 009: training_accuracy = 0.3805, testing_accuracy = 0.3988\n","           training_loss = 0.016974, testing_loss = 0.016507\n","Learning Rate = 0.10000000\n","Epoch 010: training_accuracy = 0.3710, testing_accuracy = 0.3873\n","           training_loss = 0.017603, testing_loss = 0.017099\n","Learning Rate = 0.10000000\n","Epoch 011: training_accuracy = 0.3920, testing_accuracy = 0.4132\n","           training_loss = 0.016709, testing_loss = 0.016147\n","Learning Rate = 0.10000000\n","Epoch 012: training_accuracy = 0.4040, testing_accuracy = 0.4387\n","           training_loss = 0.016912, testing_loss = 0.016434\n","Learning Rate = 0.10000000\n","Epoch 013: training_accuracy = 0.3747, testing_accuracy = 0.3965\n","           training_loss = 0.017079, testing_loss = 0.016526\n","Learning Rate = 0.10000000\n","Epoch 014: training_accuracy = 0.3474, testing_accuracy = 0.3686\n","           training_loss = 0.017532, testing_loss = 0.017205\n","Learning Rate = 0.10000000\n","Epoch 015: training_accuracy = 0.3963, testing_accuracy = 0.4206\n","           training_loss = 0.016882, testing_loss = 0.016287\n","Learning Rate = 0.10000000\n","Epoch 016: training_accuracy = 0.3884, testing_accuracy = 0.4032\n","           training_loss = 0.016857, testing_loss = 0.016560\n","Learning Rate = 0.10000000\n","Epoch 017: training_accuracy = 0.3775, testing_accuracy = 0.3884\n","           training_loss = 0.017316, testing_loss = 0.016900\n","Learning Rate = 0.10000000\n","Epoch 018: training_accuracy = 0.3750, testing_accuracy = 0.4004\n","           training_loss = 0.017159, testing_loss = 0.016665\n","Learning Rate = 0.10000000\n","Epoch 019: training_accuracy = 0.3760, testing_accuracy = 0.3957\n","           training_loss = 0.017179, testing_loss = 0.016737\n","Learning Rate = 0.10000000\n","Epoch 020: training_accuracy = 0.3760, testing_accuracy = 0.3891\n","           training_loss = 0.017742, testing_loss = 0.017377\n","Learning Rate = 0.10000000\n","Epoch 021: training_accuracy = 0.3588, testing_accuracy = 0.3730\n","           training_loss = 0.017953, testing_loss = 0.017646\n","Learning Rate = 0.10000000\n","Epoch 022: training_accuracy = 0.3426, testing_accuracy = 0.3525\n","           training_loss = 0.017922, testing_loss = 0.017636\n","Learning Rate = 0.10000000\n","Epoch 023: training_accuracy = 0.3718, testing_accuracy = 0.3928\n","           training_loss = 0.017359, testing_loss = 0.016956\n","Learning Rate = 0.10000000\n","Epoch 024: training_accuracy = 0.3768, testing_accuracy = 0.3900\n","           training_loss = 0.017637, testing_loss = 0.017233\n","Learning Rate = 0.10000000\n","Epoch 025: training_accuracy = 0.3653, testing_accuracy = 0.3716\n","           training_loss = 0.017146, testing_loss = 0.016930\n","Learning Rate = 0.10000000\n","Epoch 026: training_accuracy = 0.4088, testing_accuracy = 0.4273\n","           training_loss = 0.016594, testing_loss = 0.016083\n","Learning Rate = 0.10000000\n","Epoch 027: training_accuracy = 0.3457, testing_accuracy = 0.3632\n","           training_loss = 0.018141, testing_loss = 0.017582\n","Learning Rate = 0.10000000\n","Epoch 028: training_accuracy = 0.3626, testing_accuracy = 0.3840\n","           training_loss = 0.017312, testing_loss = 0.016882\n","Learning Rate = 0.10000000\n","Epoch 029: training_accuracy = 0.4161, testing_accuracy = 0.4352\n","           training_loss = 0.016152, testing_loss = 0.015881\n","Learning Rate = 0.10000000\n","Epoch 030: training_accuracy = 0.3722, testing_accuracy = 0.3925\n","           training_loss = 0.017107, testing_loss = 0.016612\n","Learning Rate = 0.10000000\n","Epoch 031: training_accuracy = 0.3567, testing_accuracy = 0.3694\n","           training_loss = 0.017741, testing_loss = 0.017560\n","Learning Rate = 0.10000000\n","Epoch 032: training_accuracy = 0.3883, testing_accuracy = 0.4119\n","           training_loss = 0.017198, testing_loss = 0.016791\n","Learning Rate = 0.10000000\n","Epoch 033: training_accuracy = 0.3824, testing_accuracy = 0.4016\n","           training_loss = 0.017067, testing_loss = 0.016666\n","Learning Rate = 0.10000000\n","Epoch 034: training_accuracy = 0.3788, testing_accuracy = 0.3976\n","           training_loss = 0.017338, testing_loss = 0.016763\n","Learning Rate = 0.10000000\n","Epoch 035: training_accuracy = 0.3962, testing_accuracy = 0.4055\n","           training_loss = 0.017017, testing_loss = 0.016759\n","Learning Rate = 0.10000000\n","Epoch 036: training_accuracy = 0.3440, testing_accuracy = 0.3622\n","           training_loss = 0.018223, testing_loss = 0.017896\n","Learning Rate = 0.10000000\n","Epoch 037: training_accuracy = 0.3859, testing_accuracy = 0.4009\n","           training_loss = 0.017145, testing_loss = 0.016791\n","Learning Rate = 0.10000000\n","Epoch 038: training_accuracy = 0.3776, testing_accuracy = 0.4104\n","           training_loss = 0.017567, testing_loss = 0.016846\n","Learning Rate = 0.10000000\n","Epoch 039: training_accuracy = 0.3541, testing_accuracy = 0.3709\n","           training_loss = 0.018180, testing_loss = 0.017851\n","Learning Rate = 0.10000000\n","Epoch 040: training_accuracy = 0.3800, testing_accuracy = 0.4066\n","           training_loss = 0.016984, testing_loss = 0.016505\n","Learning Rate = 0.10000000\n","Epoch 041: training_accuracy = 0.3790, testing_accuracy = 0.3974\n","           training_loss = 0.017093, testing_loss = 0.016754\n","Learning Rate = 0.10000000\n","Epoch 042: training_accuracy = 0.4054, testing_accuracy = 0.4278\n","           training_loss = 0.016584, testing_loss = 0.016164\n","Learning Rate = 0.10000000\n","Epoch 043: training_accuracy = 0.3524, testing_accuracy = 0.3696\n","           training_loss = 0.018335, testing_loss = 0.017977\n","Learning Rate = 0.10000000\n","Epoch 044: training_accuracy = 0.4061, testing_accuracy = 0.4290\n","           training_loss = 0.016489, testing_loss = 0.016020\n","Learning Rate = 0.10000000\n","Epoch 045: training_accuracy = 0.4027, testing_accuracy = 0.4209\n","           training_loss = 0.016939, testing_loss = 0.016596\n","Learning Rate = 0.10000000\n","Epoch 046: training_accuracy = 0.3976, testing_accuracy = 0.4206\n","           training_loss = 0.016794, testing_loss = 0.016207\n","Learning Rate = 0.10000000\n","Epoch 047: training_accuracy = 0.4067, testing_accuracy = 0.4305\n","           training_loss = 0.016727, testing_loss = 0.016060\n","Learning Rate = 0.10000000\n","Epoch 048: training_accuracy = 0.3907, testing_accuracy = 0.4123\n","           training_loss = 0.016767, testing_loss = 0.016390\n","Learning Rate = 0.10000000\n","Epoch 049: training_accuracy = 0.3534, testing_accuracy = 0.3721\n","           training_loss = 0.017479, testing_loss = 0.017007\n","Learning Rate = 0.10000000\n","Epoch 050: training_accuracy = 0.3962, testing_accuracy = 0.4208\n","           training_loss = 0.016975, testing_loss = 0.016418\n","Learning Rate = 0.10000000\n","Epoch 051: training_accuracy = 0.4092, testing_accuracy = 0.4341\n","           training_loss = 0.016365, testing_loss = 0.015962\n","Learning Rate = 0.10000000\n","Epoch 052: training_accuracy = 0.3965, testing_accuracy = 0.4162\n","           training_loss = 0.016730, testing_loss = 0.016283\n","Learning Rate = 0.10000000\n","Epoch 053: training_accuracy = 0.4125, testing_accuracy = 0.4314\n","           training_loss = 0.016597, testing_loss = 0.016275\n","Learning Rate = 0.10000000\n","Epoch 054: training_accuracy = 0.3851, testing_accuracy = 0.4011\n","           training_loss = 0.017281, testing_loss = 0.017073\n","Learning Rate = 0.10000000\n","Epoch 055: training_accuracy = 0.3693, testing_accuracy = 0.3938\n","           training_loss = 0.017159, testing_loss = 0.016666\n","Learning Rate = 0.10000000\n","Epoch 056: training_accuracy = 0.4231, testing_accuracy = 0.4502\n","           training_loss = 0.016560, testing_loss = 0.015999\n","Learning Rate = 0.10000000\n","Epoch 057: training_accuracy = 0.3657, testing_accuracy = 0.3759\n","           training_loss = 0.018179, testing_loss = 0.017827\n","Learning Rate = 0.10000000\n","Epoch 058: training_accuracy = 0.3983, testing_accuracy = 0.4137\n","           training_loss = 0.016865, testing_loss = 0.016637\n","Learning Rate = 0.10000000\n","Epoch 059: training_accuracy = 0.3374, testing_accuracy = 0.3566\n","           training_loss = 0.018436, testing_loss = 0.017863\n","Learning Rate = 0.10000000\n","Epoch 060: training_accuracy = 0.3771, testing_accuracy = 0.4002\n","           training_loss = 0.018144, testing_loss = 0.017692\n","Learning Rate = 0.10000000\n","Epoch 061: training_accuracy = 0.3724, testing_accuracy = 0.3931\n","           training_loss = 0.018015, testing_loss = 0.017473\n","Learning Rate = 0.10000000\n","Epoch 062: training_accuracy = 0.3804, testing_accuracy = 0.3944\n","           training_loss = 0.017264, testing_loss = 0.016855\n","Learning Rate = 0.10000000\n","Epoch 063: training_accuracy = 0.3827, testing_accuracy = 0.4053\n","           training_loss = 0.016889, testing_loss = 0.016416\n","Learning Rate = 0.10000000\n","Epoch 064: training_accuracy = 0.3736, testing_accuracy = 0.3948\n","           training_loss = 0.017349, testing_loss = 0.016945\n","Learning Rate = 0.10000000\n","Epoch 065: training_accuracy = 0.3874, testing_accuracy = 0.4048\n","           training_loss = 0.017371, testing_loss = 0.017009\n","Learning Rate = 0.10000000\n","Epoch 066: training_accuracy = 0.3978, testing_accuracy = 0.4200\n","           training_loss = 0.017355, testing_loss = 0.016711\n","Learning Rate = 0.10000000\n","Epoch 067: training_accuracy = 0.3904, testing_accuracy = 0.4096\n","           training_loss = 0.016679, testing_loss = 0.016311\n","Learning Rate = 0.10000000\n","Epoch 068: training_accuracy = 0.3741, testing_accuracy = 0.3905\n","           training_loss = 0.018012, testing_loss = 0.017403\n","Learning Rate = 0.10000000\n","Epoch 069: training_accuracy = 0.3969, testing_accuracy = 0.4154\n","           training_loss = 0.016957, testing_loss = 0.016502\n","Learning Rate = 0.10000000\n","Epoch 070: training_accuracy = 0.3954, testing_accuracy = 0.4117\n","           training_loss = 0.016767, testing_loss = 0.016350\n","Learning Rate = 0.10000000\n","Epoch 071: training_accuracy = 0.4044, testing_accuracy = 0.4222\n","           training_loss = 0.016633, testing_loss = 0.016266\n","Learning Rate = 0.10000000\n","Epoch 072: training_accuracy = 0.3993, testing_accuracy = 0.4185\n","           training_loss = 0.016570, testing_loss = 0.016139\n","Learning Rate = 0.10000000\n","Epoch 073: training_accuracy = 0.4192, testing_accuracy = 0.4473\n","           training_loss = 0.016600, testing_loss = 0.016003\n","Learning Rate = 0.10000000\n","Epoch 074: training_accuracy = 0.4260, testing_accuracy = 0.4519\n","           training_loss = 0.016575, testing_loss = 0.015942\n","Learning Rate = 0.10000000\n","Epoch 075: training_accuracy = 0.4144, testing_accuracy = 0.4328\n","           training_loss = 0.016440, testing_loss = 0.016080\n","Learning Rate = 0.10000000\n","Epoch 076: training_accuracy = 0.3890, testing_accuracy = 0.4092\n","           training_loss = 0.017256, testing_loss = 0.016939\n","Learning Rate = 0.10000000\n","Epoch 077: training_accuracy = 0.4036, testing_accuracy = 0.4297\n","           training_loss = 0.016851, testing_loss = 0.016278\n","Learning Rate = 0.10000000\n","Epoch 078: training_accuracy = 0.3849, testing_accuracy = 0.4065\n","           training_loss = 0.017147, testing_loss = 0.016627\n","Learning Rate = 0.10000000\n","Epoch 079: training_accuracy = 0.3701, testing_accuracy = 0.3865\n","           training_loss = 0.017381, testing_loss = 0.017110\n","Learning Rate = 0.01000000\n","Epoch 080: training_accuracy = 0.3849, testing_accuracy = 0.4082\n","           training_loss = 0.017282, testing_loss = 0.016688\n","Learning Rate = 0.01000000\n","Epoch 081: training_accuracy = 0.4766, testing_accuracy = 0.5022\n","           training_loss = 0.014597, testing_loss = 0.014136\n","Learning Rate = 0.01000000\n","Epoch 082: training_accuracy = 0.4918, testing_accuracy = 0.5104\n","           training_loss = 0.014211, testing_loss = 0.013798\n","Learning Rate = 0.01000000\n","Epoch 083: training_accuracy = 0.5028, testing_accuracy = 0.5256\n","           training_loss = 0.013823, testing_loss = 0.013396\n","Learning Rate = 0.01000000\n","Epoch 084: training_accuracy = 0.5114, testing_accuracy = 0.5334\n","           training_loss = 0.013745, testing_loss = 0.013258\n","Learning Rate = 0.01000000\n","Epoch 085: training_accuracy = 0.5113, testing_accuracy = 0.5347\n","           training_loss = 0.013603, testing_loss = 0.013148\n","Learning Rate = 0.01000000\n","Epoch 086: training_accuracy = 0.5202, testing_accuracy = 0.5437\n","           training_loss = 0.013364, testing_loss = 0.012962\n","Learning Rate = 0.01000000\n","Epoch 087: training_accuracy = 0.5270, testing_accuracy = 0.5479\n","           training_loss = 0.013295, testing_loss = 0.012852\n","Learning Rate = 0.01000000\n","Epoch 088: training_accuracy = 0.5299, testing_accuracy = 0.5599\n","           training_loss = 0.013210, testing_loss = 0.012662\n","Learning Rate = 0.01000000\n","Epoch 089: training_accuracy = 0.5382, testing_accuracy = 0.5642\n","           training_loss = 0.013023, testing_loss = 0.012500\n","Learning Rate = 0.01000000\n","Epoch 090: training_accuracy = 0.5385, testing_accuracy = 0.5673\n","           training_loss = 0.012964, testing_loss = 0.012441\n","Learning Rate = 0.01000000\n","Epoch 091: training_accuracy = 0.5370, testing_accuracy = 0.5652\n","           training_loss = 0.012866, testing_loss = 0.012416\n","Learning Rate = 0.01000000\n","Epoch 092: training_accuracy = 0.5433, testing_accuracy = 0.5694\n","           training_loss = 0.012908, testing_loss = 0.012373\n","Learning Rate = 0.01000000\n","Epoch 093: training_accuracy = 0.5512, testing_accuracy = 0.5767\n","           training_loss = 0.012609, testing_loss = 0.012244\n","Learning Rate = 0.01000000\n","Epoch 094: training_accuracy = 0.5589, testing_accuracy = 0.5833\n","           training_loss = 0.012488, testing_loss = 0.012047\n","Learning Rate = 0.01000000\n","Epoch 095: training_accuracy = 0.5579, testing_accuracy = 0.5810\n","           training_loss = 0.012429, testing_loss = 0.012010\n","Learning Rate = 0.01000000\n","Epoch 096: training_accuracy = 0.5603, testing_accuracy = 0.5838\n","           training_loss = 0.012398, testing_loss = 0.011962\n","Learning Rate = 0.01000000\n","Epoch 097: training_accuracy = 0.5576, testing_accuracy = 0.5830\n","           training_loss = 0.012296, testing_loss = 0.011900\n","Learning Rate = 0.01000000\n","Epoch 098: training_accuracy = 0.5611, testing_accuracy = 0.5851\n","           training_loss = 0.012293, testing_loss = 0.011907\n","Learning Rate = 0.01000000\n","Epoch 099: training_accuracy = 0.5747, testing_accuracy = 0.5966\n","           training_loss = 0.012000, testing_loss = 0.011649\n","Learning Rate = 0.01000000\n","Epoch 100: training_accuracy = 0.5736, testing_accuracy = 0.5916\n","           training_loss = 0.012047, testing_loss = 0.011613\n","Learning Rate = 0.01000000\n","Epoch 101: training_accuracy = 0.5711, testing_accuracy = 0.5904\n","           training_loss = 0.012022, testing_loss = 0.011579\n","Learning Rate = 0.01000000\n","Epoch 102: training_accuracy = 0.5786, testing_accuracy = 0.6032\n","           training_loss = 0.011923, testing_loss = 0.011513\n","Learning Rate = 0.01000000\n","Epoch 103: training_accuracy = 0.5745, testing_accuracy = 0.6012\n","           training_loss = 0.011973, testing_loss = 0.011485\n","Learning Rate = 0.01000000\n","Epoch 104: training_accuracy = 0.5839, testing_accuracy = 0.6065\n","           training_loss = 0.011795, testing_loss = 0.011336\n","Learning Rate = 0.01000000\n","Epoch 105: training_accuracy = 0.5867, testing_accuracy = 0.6061\n","           training_loss = 0.011736, testing_loss = 0.011304\n","Learning Rate = 0.01000000\n","Epoch 106: training_accuracy = 0.5847, testing_accuracy = 0.6121\n","           training_loss = 0.011620, testing_loss = 0.011150\n","Learning Rate = 0.01000000\n","Epoch 107: training_accuracy = 0.5856, testing_accuracy = 0.6062\n","           training_loss = 0.011681, testing_loss = 0.011327\n","Learning Rate = 0.01000000\n","Epoch 108: training_accuracy = 0.5929, testing_accuracy = 0.6191\n","           training_loss = 0.011491, testing_loss = 0.010970\n","Learning Rate = 0.01000000\n","Epoch 109: training_accuracy = 0.5984, testing_accuracy = 0.6175\n","           training_loss = 0.011452, testing_loss = 0.011047\n","Learning Rate = 0.01000000\n","Epoch 110: training_accuracy = 0.5996, testing_accuracy = 0.6254\n","           training_loss = 0.011319, testing_loss = 0.010848\n","Learning Rate = 0.01000000\n","Epoch 111: training_accuracy = 0.5948, testing_accuracy = 0.6178\n","           training_loss = 0.011420, testing_loss = 0.010986\n","Learning Rate = 0.01000000\n","Epoch 112: training_accuracy = 0.5915, testing_accuracy = 0.6151\n","           training_loss = 0.011520, testing_loss = 0.011186\n","Learning Rate = 0.01000000\n","Epoch 113: training_accuracy = 0.6036, testing_accuracy = 0.6245\n","           training_loss = 0.011253, testing_loss = 0.010828\n","Learning Rate = 0.01000000\n","Epoch 114: training_accuracy = 0.5917, testing_accuracy = 0.6140\n","           training_loss = 0.011663, testing_loss = 0.011256\n","Learning Rate = 0.01000000\n","Epoch 115: training_accuracy = 0.6096, testing_accuracy = 0.6341\n","           training_loss = 0.011127, testing_loss = 0.010644\n","Learning Rate = 0.01000000\n","Epoch 116: training_accuracy = 0.6057, testing_accuracy = 0.6259\n","           training_loss = 0.011188, testing_loss = 0.010822\n","Learning Rate = 0.01000000\n","Epoch 117: training_accuracy = 0.6146, testing_accuracy = 0.6375\n","           training_loss = 0.010956, testing_loss = 0.010505\n","Learning Rate = 0.01000000\n","Epoch 118: training_accuracy = 0.6155, testing_accuracy = 0.6336\n","           training_loss = 0.010921, testing_loss = 0.010502\n","Learning Rate = 0.01000000\n","Epoch 119: training_accuracy = 0.6177, testing_accuracy = 0.6383\n","           training_loss = 0.010916, testing_loss = 0.010498\n","Learning Rate = 0.01000000\n","Epoch 120: training_accuracy = 0.6177, testing_accuracy = 0.6437\n","           training_loss = 0.010845, testing_loss = 0.010340\n","Learning Rate = 0.01000000\n","Epoch 121: training_accuracy = 0.6079, testing_accuracy = 0.6341\n","           training_loss = 0.010982, testing_loss = 0.010541\n","Learning Rate = 0.01000000\n","Epoch 122: training_accuracy = 0.6196, testing_accuracy = 0.6459\n","           training_loss = 0.010798, testing_loss = 0.010355\n","Learning Rate = 0.01000000\n","Epoch 123: training_accuracy = 0.6170, testing_accuracy = 0.6439\n","           training_loss = 0.010913, testing_loss = 0.010399\n","Learning Rate = 0.01000000\n","Epoch 124: training_accuracy = 0.6174, testing_accuracy = 0.6462\n","           training_loss = 0.010814, testing_loss = 0.010304\n","Learning Rate = 0.01000000\n","Epoch 125: training_accuracy = 0.6312, testing_accuracy = 0.6551\n","           training_loss = 0.010517, testing_loss = 0.010072\n","Learning Rate = 0.01000000\n","Epoch 126: training_accuracy = 0.6159, testing_accuracy = 0.6431\n","           training_loss = 0.010842, testing_loss = 0.010390\n","Learning Rate = 0.01000000\n","Epoch 127: training_accuracy = 0.6193, testing_accuracy = 0.6411\n","           training_loss = 0.010857, testing_loss = 0.010587\n","Learning Rate = 0.01000000\n","Epoch 128: training_accuracy = 0.6083, testing_accuracy = 0.6364\n","           training_loss = 0.011169, testing_loss = 0.010586\n","Learning Rate = 0.01000000\n","Epoch 129: training_accuracy = 0.6262, testing_accuracy = 0.6543\n","           training_loss = 0.010690, testing_loss = 0.010089\n","Learning Rate = 0.00100000\n","Epoch 130: training_accuracy = 0.6329, testing_accuracy = 0.6565\n","           training_loss = 0.010400, testing_loss = 0.009998\n","Learning Rate = 0.00100000\n","Epoch 131: training_accuracy = 0.6478, testing_accuracy = 0.6676\n","           training_loss = 0.010035, testing_loss = 0.009621\n","Learning Rate = 0.00100000\n","Epoch 132: training_accuracy = 0.6477, testing_accuracy = 0.6683\n","           training_loss = 0.010019, testing_loss = 0.009611\n","Learning Rate = 0.00100000\n","Epoch 133: training_accuracy = 0.6457, testing_accuracy = 0.6681\n","           training_loss = 0.010041, testing_loss = 0.009556\n","Learning Rate = 0.00100000\n","Epoch 134: training_accuracy = 0.6483, testing_accuracy = 0.6692\n","           training_loss = 0.009972, testing_loss = 0.009539\n","Learning Rate = 0.00100000\n","Epoch 135: training_accuracy = 0.6497, testing_accuracy = 0.6700\n","           training_loss = 0.009939, testing_loss = 0.009492\n","Learning Rate = 0.00100000\n","Epoch 136: training_accuracy = 0.6511, testing_accuracy = 0.6708\n","           training_loss = 0.009939, testing_loss = 0.009498\n","Learning Rate = 0.00100000\n","Epoch 137: training_accuracy = 0.6506, testing_accuracy = 0.6739\n","           training_loss = 0.009940, testing_loss = 0.009465\n","Learning Rate = 0.00100000\n","Epoch 138: training_accuracy = 0.6505, testing_accuracy = 0.6721\n","           training_loss = 0.009898, testing_loss = 0.009468\n","Learning Rate = 0.00100000\n","Epoch 139: training_accuracy = 0.6520, testing_accuracy = 0.6722\n","           training_loss = 0.009900, testing_loss = 0.009475\n","Learning Rate = 0.00100000\n","Epoch 140: training_accuracy = 0.6518, testing_accuracy = 0.6729\n","           training_loss = 0.009871, testing_loss = 0.009468\n","Learning Rate = 0.00100000\n","Epoch 141: training_accuracy = 0.6501, testing_accuracy = 0.6718\n","           training_loss = 0.009895, testing_loss = 0.009443\n","Learning Rate = 0.00100000\n","Epoch 142: training_accuracy = 0.6515, testing_accuracy = 0.6742\n","           training_loss = 0.009886, testing_loss = 0.009443\n","Learning Rate = 0.00100000\n","Epoch 143: training_accuracy = 0.6530, testing_accuracy = 0.6725\n","           training_loss = 0.009855, testing_loss = 0.009428\n","Learning Rate = 0.00100000\n","Epoch 144: training_accuracy = 0.6521, testing_accuracy = 0.6736\n","           training_loss = 0.009809, testing_loss = 0.009451\n","Learning Rate = 0.00100000\n","Epoch 145: training_accuracy = 0.6544, testing_accuracy = 0.6752\n","           training_loss = 0.009815, testing_loss = 0.009410\n","Learning Rate = 0.00100000\n","Epoch 146: training_accuracy = 0.6542, testing_accuracy = 0.6735\n","           training_loss = 0.009797, testing_loss = 0.009452\n","Learning Rate = 0.00100000\n","Epoch 147: training_accuracy = 0.6540, testing_accuracy = 0.6738\n","           training_loss = 0.009800, testing_loss = 0.009405\n","Learning Rate = 0.00100000\n","Epoch 148: training_accuracy = 0.6560, testing_accuracy = 0.6761\n","           training_loss = 0.009788, testing_loss = 0.009405\n","Learning Rate = 0.00100000\n","Epoch 149: training_accuracy = 0.6576, testing_accuracy = 0.6736\n","           training_loss = 0.009768, testing_loss = 0.009354\n","Learning Rate = 0.00100000\n","Epoch 150: training_accuracy = 0.6558, testing_accuracy = 0.6755\n","           training_loss = 0.009793, testing_loss = 0.009394\n","Learning Rate = 0.00100000\n","Epoch 151: training_accuracy = 0.6546, testing_accuracy = 0.6750\n","           training_loss = 0.009813, testing_loss = 0.009388\n","Learning Rate = 0.00100000\n","Epoch 152: training_accuracy = 0.6539, testing_accuracy = 0.6742\n","           training_loss = 0.009790, testing_loss = 0.009396\n","Learning Rate = 0.00100000\n","Epoch 153: training_accuracy = 0.6577, testing_accuracy = 0.6751\n","           training_loss = 0.009694, testing_loss = 0.009344\n","Learning Rate = 0.00100000\n","Epoch 154: training_accuracy = 0.6563, testing_accuracy = 0.6726\n","           training_loss = 0.009802, testing_loss = 0.009395\n","Learning Rate = 0.00100000\n","Epoch 155: training_accuracy = 0.6551, testing_accuracy = 0.6738\n","           training_loss = 0.009800, testing_loss = 0.009338\n","Learning Rate = 0.00100000\n","Epoch 156: training_accuracy = 0.6577, testing_accuracy = 0.6784\n","           training_loss = 0.009742, testing_loss = 0.009289\n","Learning Rate = 0.00100000\n","Epoch 157: training_accuracy = 0.6579, testing_accuracy = 0.6762\n","           training_loss = 0.009723, testing_loss = 0.009339\n","Learning Rate = 0.00100000\n","Epoch 158: training_accuracy = 0.6605, testing_accuracy = 0.6756\n","           training_loss = 0.009704, testing_loss = 0.009304\n","Learning Rate = 0.00100000\n","Epoch 159: training_accuracy = 0.6581, testing_accuracy = 0.6732\n","           training_loss = 0.009731, testing_loss = 0.009332\n","Learning Rate = 0.00100000\n","Epoch 160: training_accuracy = 0.6582, testing_accuracy = 0.6763\n","           training_loss = 0.009679, testing_loss = 0.009316\n","Learning Rate = 0.00100000\n","Epoch 161: training_accuracy = 0.6588, testing_accuracy = 0.6795\n","           training_loss = 0.009683, testing_loss = 0.009296\n","Learning Rate = 0.00100000\n","Epoch 162: training_accuracy = 0.6578, testing_accuracy = 0.6792\n","           training_loss = 0.009669, testing_loss = 0.009264\n","Learning Rate = 0.00100000\n","Epoch 163: training_accuracy = 0.6578, testing_accuracy = 0.6764\n","           training_loss = 0.009636, testing_loss = 0.009303\n","Learning Rate = 0.00100000\n","Epoch 164: training_accuracy = 0.6595, testing_accuracy = 0.6797\n","           training_loss = 0.009699, testing_loss = 0.009287\n","Learning Rate = 0.00100000\n","Epoch 165: training_accuracy = 0.6598, testing_accuracy = 0.6781\n","           training_loss = 0.009687, testing_loss = 0.009261\n","Learning Rate = 0.00100000\n","Epoch 166: training_accuracy = 0.6590, testing_accuracy = 0.6783\n","           training_loss = 0.009664, testing_loss = 0.009288\n","Learning Rate = 0.00100000\n","Epoch 167: training_accuracy = 0.6594, testing_accuracy = 0.6804\n","           training_loss = 0.009686, testing_loss = 0.009281\n","Learning Rate = 0.00100000\n","Epoch 168: training_accuracy = 0.6626, testing_accuracy = 0.6807\n","           training_loss = 0.009602, testing_loss = 0.009231\n","Learning Rate = 0.00100000\n","Epoch 169: training_accuracy = 0.6624, testing_accuracy = 0.6787\n","           training_loss = 0.009566, testing_loss = 0.009249\n","Learning Rate = 0.00010000\n","Epoch 170: training_accuracy = 0.6592, testing_accuracy = 0.6819\n","           training_loss = 0.009669, testing_loss = 0.009232\n","Learning Rate = 0.00010000\n","Epoch 171: training_accuracy = 0.6643, testing_accuracy = 0.6806\n","           training_loss = 0.009511, testing_loss = 0.009216\n","Learning Rate = 0.00010000\n","Epoch 172: training_accuracy = 0.6624, testing_accuracy = 0.6817\n","           training_loss = 0.009638, testing_loss = 0.009195\n","Learning Rate = 0.00010000\n","Epoch 173: training_accuracy = 0.6630, testing_accuracy = 0.6828\n","           training_loss = 0.009588, testing_loss = 0.009197\n","Learning Rate = 0.00010000\n","Epoch 174: training_accuracy = 0.6592, testing_accuracy = 0.6825\n","           training_loss = 0.009634, testing_loss = 0.009204\n","Learning Rate = 0.00010000\n","Epoch 175: training_accuracy = 0.6616, testing_accuracy = 0.6814\n","           training_loss = 0.009592, testing_loss = 0.009192\n","Learning Rate = 0.00010000\n","Epoch 176: training_accuracy = 0.6621, testing_accuracy = 0.6819\n","           training_loss = 0.009584, testing_loss = 0.009191\n","Learning Rate = 0.00010000\n","Epoch 177: training_accuracy = 0.6604, testing_accuracy = 0.6821\n","           training_loss = 0.009626, testing_loss = 0.009191\n","Learning Rate = 0.00010000\n","Epoch 178: training_accuracy = 0.6647, testing_accuracy = 0.6810\n","           training_loss = 0.009543, testing_loss = 0.009188\n","Learning Rate = 0.00010000\n","Epoch 179: training_accuracy = 0.6624, testing_accuracy = 0.6825\n","           training_loss = 0.009543, testing_loss = 0.009196\n","Learning Rate = 0.00010000\n","Epoch 180: training_accuracy = 0.6626, testing_accuracy = 0.6829\n","           training_loss = 0.009574, testing_loss = 0.009198\n","Learning Rate = 0.00010000\n","Epoch 181: training_accuracy = 0.6624, testing_accuracy = 0.6821\n","           training_loss = 0.009580, testing_loss = 0.009187\n","Learning Rate = 0.00010000\n","Epoch 182: training_accuracy = 0.6628, testing_accuracy = 0.6817\n","           training_loss = 0.009568, testing_loss = 0.009193\n","Learning Rate = 0.00010000\n","Epoch 183: training_accuracy = 0.6652, testing_accuracy = 0.6822\n","           training_loss = 0.009566, testing_loss = 0.009195\n","Learning Rate = 0.00010000\n","Epoch 184: training_accuracy = 0.6607, testing_accuracy = 0.6830\n","           training_loss = 0.009635, testing_loss = 0.009183\n","Learning Rate = 0.00010000\n","Epoch 185: training_accuracy = 0.6615, testing_accuracy = 0.6830\n","           training_loss = 0.009585, testing_loss = 0.009185\n","Learning Rate = 0.00010000\n","Epoch 186: training_accuracy = 0.6609, testing_accuracy = 0.6818\n","           training_loss = 0.009592, testing_loss = 0.009201\n","Learning Rate = 0.00010000\n","Epoch 187: training_accuracy = 0.6664, testing_accuracy = 0.6811\n","           training_loss = 0.009538, testing_loss = 0.009193\n","Learning Rate = 0.00010000\n","Epoch 188: training_accuracy = 0.6649, testing_accuracy = 0.6835\n","           training_loss = 0.009556, testing_loss = 0.009181\n","Learning Rate = 0.00010000\n","Epoch 189: training_accuracy = 0.6642, testing_accuracy = 0.6824\n","           training_loss = 0.009577, testing_loss = 0.009186\n","Learning Rate = 0.00010000\n","Epoch 190: training_accuracy = 0.6626, testing_accuracy = 0.6832\n","           training_loss = 0.009566, testing_loss = 0.009181\n","Learning Rate = 0.00010000\n","Epoch 191: training_accuracy = 0.6630, testing_accuracy = 0.6836\n","           training_loss = 0.009564, testing_loss = 0.009178\n","Learning Rate = 0.00010000\n","Epoch 192: training_accuracy = 0.6645, testing_accuracy = 0.6832\n","           training_loss = 0.009546, testing_loss = 0.009171\n","Learning Rate = 0.00010000\n","Epoch 193: training_accuracy = 0.6621, testing_accuracy = 0.6813\n","           training_loss = 0.009610, testing_loss = 0.009178\n","Learning Rate = 0.00010000\n","Epoch 194: training_accuracy = 0.6616, testing_accuracy = 0.6820\n","           training_loss = 0.009591, testing_loss = 0.009183\n","Learning Rate = 0.00010000\n","Epoch 195: training_accuracy = 0.6616, testing_accuracy = 0.6825\n","           training_loss = 0.009549, testing_loss = 0.009180\n","Learning Rate = 0.00010000\n","Epoch 196: training_accuracy = 0.6633, testing_accuracy = 0.6824\n","           training_loss = 0.009563, testing_loss = 0.009183\n","Learning Rate = 0.00010000\n","Epoch 197: training_accuracy = 0.6641, testing_accuracy = 0.6833\n","           training_loss = 0.009530, testing_loss = 0.009187\n","Learning Rate = 0.00010000\n","Epoch 198: training_accuracy = 0.6636, testing_accuracy = 0.6821\n","           training_loss = 0.009578, testing_loss = 0.009180\n","Learning Rate = 0.00010000\n","Epoch 199: training_accuracy = 0.6627, testing_accuracy = 0.6820\n","           training_loss = 0.009554, testing_loss = 0.009190\n","Learning Rate = 0.00010000\n","Epoch 200: training_accuracy = 0.6641, testing_accuracy = 0.6822\n","           training_loss = 0.009553, testing_loss = 0.009183\n","Finished Training after 200 iterations\n"],"name":"stdout"}]}]}